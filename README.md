# simple_agent

*tl;dr*
A simple, set of classes for 
* Rate limited free models (Google AI Studio)
* Basic, transparent agent and memory objects (hackable as needed)
* Basic "World" object to provide agents a sandbox where they can interact with an external environment

# 1. Model Wrapper

## Documentation
*documentation generated by o3*

A lightweight wrapper to let you focus on *ideas*, not boiler‑plate. You get:
* **Text & multimodal prompts** (images, videos, URLs)
* **Function‑calling** via simple `register_tool()`
* Built‑in **rate‑limit smoothing** so you don’t hit the free‑tier wall

---

## 1 · Setup (once)
```bash
pip install google-generativeai
```
```python
from google.colab import userdata  # Colab only
import os
os.environ["GEMINI_API_KEY"] = userdata.get("GEMINI_API_KEY")
```
*(In plain Python, just `export GEMINI_API_KEY="<your‑key>"` in the shell.)*

---

## 2 · Create a model object
```python
from gemini_wrapper import GeminiModel  # or the file name you saved

freemodel = GeminiModel(
    model_name="gemini-2.0-flash-lite",  # see console for other names
    rate_limit_minutes=30,               # free‑tier: 30 req/min
    rate_limit_daily=1000               # free‑tier: 1 000 req/day
)
```
That’s it—`freemodel` is your personal assistant.

---

## 3 · Plain‑text questions
```python
resp = freemodel.generate_content(
    user_prompt="Explain photosynthesis in 2 sentences."
)
print(resp.text)
```

---

## 4 · Multimodal questions (images / video / URLs)
```python
resp = freemodel.multimodal_query(
    user_prompt="What’s happening in this picture?",
    attachment_names=["https://hips.hearstapps.com/hmg-prod/images/wisteria-in-bloom-royalty-free-image-1653423554.jpg"]
)
print(resp.text)
```
*Pass either local filenames* (e.g. `"my_plot.png"`) *or HTTP/HTTPS URLs.*

---

## 5 · Calling your own Python tools
1. **Write a normal function**
```python
def get_biography(first_name: str, last_name: str) -> str:
    return f"{first_name} {last_name} was born …"
```
2. **Register it**
```python
bio_tool = freemodel.register_tool(
    func=get_biography,
    description="Returns a short biography for a given person.",
    parameters={
        "first_name": {"type": "string", "description": "The person’s given name."},
        "last_name" : {"type": "string", "description": "The person’s family name."}
    }
)
```
3. **Ask the model**
```python
resp = freemodel.generate_content(
    user_prompt="Give me a one-line bio for Albert Einstein.",
    tools=[bio_tool]
)
```
4. **Execute any tool calls & print final answer**
```python
results = freemodel.apply_tool(resp)
print(results)
```
The wrapper detects `response.function_calls`, runs the right Python function, and hands back the results.

---

## 6 · Tips & Limits
* **Rate‑limits handled** – the wrapper pauses just enough to stay under 30 req/min.
* **Descriptions matter** – Gemini needs full‑sentence parameter descriptions.
* Use **small file sizes** for images (≤ 5 MB) to keep queries snappy.
* Stick to **JSON‑serialisable** argument types (strings, numbers, booleans).

---

## 7 · Troubleshooting cheatsheet
| Symptom | Likely cause | Fix |
|---------|--------------|-----|
| `Error: no query given` | Forgot the `user_prompt` kwarg | Provide it 😄 |
| `Tool X not registered` | Typo in tool name | Check `register_tool` call |
| Long pause before answer | Rate limiting | It’s automatic—just wait |

---

## 8 · Method reference (one‑liners)
| Method | What it does |
|--------|--------------|
| `generate_content(user_prompt, tools=None)` | Quick text answer (optionally with tools) |
| `multimodal_query(user_prompt, attachment_names=[])` | Answer with images / video included |
| `register_tool(func, description, parameters)` | Wrap a Python function for LLM use |
| `apply_tool(response)` | Execute any tool calls & return `(result, call)` list |
| `print_response(response)` | Convenience printer while debugging |


# Simple Agent Implementation

1. Chat function to query LLM
2. Simple Agent and Memory classes
3. Simple Framework to store environment state (e.g. World)

## Documentation

## 9 · SimpleAgent — one‑file conversational actor

The **`SimpleAgent`** class wraps a `GeminiModel` to produce chatty replies while keeping a tiny personal memory. No `Location` or `World` objects are required unless you add them later.

### Anatomy of `SimpleAgent`

| Attribute | Purpose | Type |
|-----------|---------|------|
| `name`    | Display name printed with each utterance | `str` |
| `traits`  | Free‑text personality/quirks | `str` |
| `status`  | Current activity / mood snippet | `str` |
| `memory`  | Rolling log of past events | `SimpleMemory` |
| `model`   | LLM interface (defaults to *gemini‑2.0‑flash‑lite*) | `GeminiModel` |

| Method | What it does | Returns |
|--------|--------------|---------|
| `respond(observation)` | Builds a prompt, calls Gemini, parses for `SAY:` or `GOODBYE:` | `(continue_bool, [speaker, text])` |
| `description()` | Pretty one‑pager of name / traits / status | `str` |
| `set_location(location)` | Placeholder for future location feature | `None` |

#### Minimal usage
```python
from gemini_wrapper import SimpleAgent, SimpleMemory

ava = SimpleAgent(
    name   = "Ava",
    traits = "helpful, concise",
    status = "idle",
    memory = SimpleMemory()
)

keep, (speaker, reply) = ava.respond(
    observation="Hi Ava! What's the capital of France?"
)
print(f"{speaker}: {reply}")
```

#### Building a chat loop
```python
def chat(agent: SimpleAgent):
    keep = True
    while keep:
        user_input = input("You: ")
        keep, (speaker, answer) = agent.respond(observation=user_input)
        print(f"{speaker}: {answer}")

chat(ava)
```

---

## 10 · Multi‑Agent conversation

Wire together two (or more) **`SimpleAgent`** instances by feeding each agent’s line into the next agent’s `respond()` call.

```python
from gemini_wrapper import SimpleAgent, SimpleMemory

alice = SimpleAgent("Alice", "curious",     "waiting", SimpleMemory())
bob   = SimpleAgent("Bob",   "analytical",  "idle",    SimpleMemory())

def converse(a, b, max_turns=10):
    utterance = "Hello!"
    keep = True
    turn = 0
    while keep and turn < max_turns:
        speaker = a if turn % 2 == 0 else b
        keep, (who, utterance) = speaker.respond(observation=utterance)
        print(f"{who}: {utterance}")
        turn += 1
    print("\n⟡ Conversation ended ⟡")

converse(alice, bob)
```

Scaling to *N* agents:
```python
agents = [alice, bob, charlie, diana]
utterance = "Hi everyone!"
keep, turn = True, 0
while keep and turn < 20:
    speaker = agents[turn % len(agents)]
    keep, (_, utterance) = speaker.respond(observation=utterance)
    print(f"{speaker.name}: {utterance}")
    turn += 1
```

---

## 11 · Creating scenario context & background

To **train or test a specialised agent**—e.g. a customer‑service rep—seed fixed knowledge and realistic customer profiles **before** dialogue begins.

### 1️⃣ Product catalogue (ground‑truth knowledge)

| Key | Example | Notes |
|-----|---------|-------|
| `name` | `"ACME X100 Coffee Maker"` | Unique identifier |
| `issues` | `["leaks", "won’t power on"]` | Common problems |
| `resolutions` | `{ "leaks": "Check gasket …", "won’t power on": "Verify outlet …" }` | One‑step fix or RMA flow |

Load with a system prompt *or* via a `lookup_product_issue()` tool.

### 2️⃣ CSR & Customer agents (seeded memories)

| Agent | Memory seeds | Why? |
|-------|--------------|------|
| **CSR** | • “You are a friendly ACME support rep.”<br>• “You can troubleshoot X100 leaks by checking the gasket …” | Instant product expertise |
| **Customer** | • “You bought an ACME X100 three months ago.”<br>• “Your machine started leaking yesterday.” | Keeps customer consistent |

```python
from gemini_wrapper import SimpleAgent, SimpleMemory

csr_memory = SimpleMemory([
    "You are an ACME customer‑service representative.",
    "You can resolve X100 leaks by instructing users to check the gasket and tighten it.",
])

cust_memory = SimpleMemory([
    "You own an ACME X100 coffee maker purchased 3 months ago.",
    "Your main issue: the unit is leaking during brewing.",
    "Today is your birthday and you’re mildly upset.",
])

csr      = SimpleAgent("CSR‑Bot",    "patient, empathetic", "on‑shift", csr_memory)
customer = SimpleAgent("Customer‑42", "frustrated",         "needs help", cust_memory)
```

### 3️⃣ Running the scenario

```python
utterance = "Hi, my ACME X100 is leaking all over the counter!"
turn, keep = 0, True
while keep and turn < 12:
    agent = customer if turn % 2 == 0 else csr
    keep, (who, utterance) = agent.respond(observation=utterance)
    print(f"{who}: {utterance}")
    turn += 1
```

Agents store the dialogue automatically in their private memories.

---

## 12 · Adding Tools for outcome actions

Gemini tools let the model **trigger real Python functions**—great for hang‑ups, resolution confirmations, or refunds.

### 12.1 Backend functions

```python
def customer_hang_up():
    print("⚠️  Customer disconnected in anger!")
    return "Customer has hung up."

def customer_accept():
    print("✅ Issue resolved to customer’s satisfaction.")
    return "Customer is happy—call complete."

def issue_refund(order_id: str):
    print(f"💸 Refund issued for order {order_id}.")
    return f"Refund processed for order {order_id}."
```

### 12.2 Register with the model

```python
hangup_tool = csr.model.register_tool(
    func=customer_hang_up,
    description="Marks the call as failed when the customer disconnects angrily.",
    parameters={}
)

accept_tool = csr.model.register_tool(
    func=customer_accept,
    description="Marks the ticket resolved when the customer is satisfied.",
    parameters={}
)

refund_tool = csr.model.register_tool(
    func=issue_refund,
    description="Processes a refund for a given order ID.",
    parameters={ "order_id": { "type": "string", "description": "E‑commerce order number." } }
)

csr_tools      = [hangup_tool, accept_tool, refund_tool]
customer_tools = [hangup_tool, accept_tool]   # customer can’t refund
```

### 12.3 Use tools during `respond()`

```python
keep, (who, utterance) = csr.respond(
    observation=prev_line,
    tools=csr_tools
)

for result, call in csr.model.apply_tool(csr.model.last_response):
    print("TOOL RESULT:", result)
```

### 12.4 Prompt guidance

Mention in the system prompt:

> “If the customer indicates anger and wants to end the call, call `customer_hang_up`.  
>  If they’re satisfied, call `customer_accept`.  
>  If a refund is appropriate, call `issue_refund` with the order ID provided.”


# Simple Sandbox/World classes

## Documentation


This guide covers the **refactored `World` class** that now exposes two ready‑made
methods—`move()` and `describe_room()`—that can be surfaced to language‑model
agents as Gemini **tools**.

---

## 13 World at a glance
```python
class World:
    locations: list[Location]
    agents:    list[SimpleAgent]

    def move(agent_name: str, dest_name: str) -> str: ...
    def describe_room(agent_name: str) -> str: ...
```
* **`locations`** – every `Location` object (rooms, zones, etc.)  
* **`agents`** – every `SimpleAgent` currently in the game  

---

## 13.1 Core helper methods

| Method | Purpose |
| ------ | ------- |
| `clear()` | Wipe all locations & agents (fresh start). |
| `add_location(location)` | Append a `Location` you made manually. |
| `add_agent(agent)` | Append a pre‑constructed `SimpleAgent`. |
| `create_world_from_names(names, agents)` | Build all locations & agents from plain data (see §3). |
| `read_adjacency_list(pairs)` | Mark rooms as neighbours, given `[("living","kitchen"), …]`. |
| `print()` | Dump every room and its inhabitants in a friendly format. |

---

## 13.2 Fast world‑building

```python
location_names = ["living", "kitchen", "garden"]

agents = [
    {"name": "Susan", "traits": "tired",  "status": "working", "location": "living"},
    {"name": "Bob",   "traits": "lazy",   "status": "cooking", "location": "kitchen"},
]

world = World()
world.create_world_from_names(location_names, agents)

# Optional: wire up room connections
adjacent = [("living","kitchen"), ("living","garden"), ("kitchen","garden")]
world.read_adjacency_list(adjacent)

world.print()
```
`create_world_from_names()` **expects** each agent‑dict to include  
`name`, `traits`, `status`, and `location`.

---

## 14 Built‑in room actions (tool‑ready)

### 14.1 `move(agent_name, dest_name) → str`
Checks adjacency and moves the agent if legal.

### 14.2 `describe_room(agent_name) → str`
Returns a one‑liner with the caller’s current room and who’s there.

---

## 15 Exposing actions as Gemini tools
```python
def move_to_location(agent_name: str, dest: str) -> str:
    return GAME_WORLD.move(agent_name, dest)

def get_current_room_description(agent_name: str) -> str:
    return GAME_WORLD.describe_room(agent_name)

move_tool = gm.register_tool(
    func=move_to_location,
    description="Moves you to an adjacent room.",
    parameters={
        "agent_name": {"type": "string", "description": "Your own name."},
        "dest":       {"type": "string", "description": "An adjacent room."}
    }
)

describe_tool = gm.register_tool(
    func=get_current_room_description,
    description="Describes your current room and who is there.",
    parameters={
        "agent_name": {"type": "string", "description": "Your own name."}
    }
)

AGENT_TOOLS = [move_tool, describe_tool]
for agent in GAME_WORLD.agents:
    agent.tools_available = AGENT_TOOLS
```

---

## 16 Prompt snippet for the LLM
```
You can navigate the house.
• To move, call move_to_location with your name and the destination.
• To look around, call get_current_room_description.
Choose a tool call before free‑text if an action is needed.
```

*Version: April 2025*
